---
layout: post
title:  Transformer distiled, Part 2 of 2
excerpt: regularizarion, self vs cross attention, adding vs concatenating positional encoding
categories: [ML, Transformer]
---

# Regularization

# Self- vs cross-attention

# Adding vs concatenating positional encoding
