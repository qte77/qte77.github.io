---
layout: post
title:  Transformer distiled, Part 2 of 2
categories: [HTML,Code]
excerpt: regularizarion, matmul vs einsum, self vs cross attention, adding vs concatenating positional encoding
categories: [ML, Transformer]
---

# Regularization

# Matmul vs einsum

# Self- vs cross-attention

# Adding vs concatenating positional encoding
