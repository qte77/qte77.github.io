---
layout: post
title:  Transformer distiled, Part 2 of 2
excerpt: 
categories: [ml, theory, transformer, regularization, attention, embedding, encoding]
---

# Transformer distiled, Part 2 of 2

## Regularization

## Self- vs Cross-Attention

## Adding vs concatenating positional encoding
