---
layout: post
title:  Transformer distiled, Part 2 of 2
excerpt: 
categories: [ml, theory, transformer, regularization, attention, embedding, encoding]
---

# Regularization

# Self- vs cross-attention

# Adding vs concatenating positional encoding
