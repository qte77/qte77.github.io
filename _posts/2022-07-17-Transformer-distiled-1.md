---
layout: post
title:  Transformer distiled, Part 1 of 2
categories: [HTML,Code]
excerpt: scaled dot-product, softmax and multi-head attention, linear layers, learned embeddings
keywords: transformer, attention, dot-product, softmax, linear layers, learned embeddings
---

# Scaled dot-product

# Softmax and multi-head attention

# Linear layers

# Learned Embeddings
